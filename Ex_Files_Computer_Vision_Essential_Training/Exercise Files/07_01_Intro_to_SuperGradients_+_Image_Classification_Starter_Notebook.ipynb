{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJw-BRX5pzS0"
      },
      "source": [
        "# üëãüèΩ What's up! It's [Harpreet](https://twitter.com/DataScienceHarp)\n",
        "\n",
        "I'll be guiding you through this notebook. At any point, if you get stuck or have questions, there are three ways to get in touch:\n",
        "\n",
        "1) Send me an email with your issue: harpreet.sahota@deci.ai\n",
        "\n",
        "2) Hop into the [Deep Learning Daily (powered by Deci) Discord server](https://discord.gg/p9ecgRhDR8), and let me know what your question is.\n",
        "\n",
        "3) [Open an issue on GitHub](https://github.com/Deci-AI/super-gradients/issues/new/choose)\n",
        "\n",
        "---\n",
        "\n",
        "This tutorial introduces [SuperGradients](https://github.com/Deci-AI/super-gradients) and how to use pre-trained models for image classification. Pre-trained models offer excellent performance with minimal effort, as they have already learned visual features from large datasets.\n",
        "\n",
        "You'll use a model pretrained on ImageNet from the SuperGradients model zoo and fine-tune it on the MiniPlaces dataset.\n",
        "\n",
        "This is a barebones introduction to using SuperGradients. It should get you familiar with the core concepts. If you need more details on SuperGradients you can [read the docs](https://www.supergradients.com/).\n",
        "\n",
        "\n",
        "# üë®üèΩ‚Äçüîß Setting up the Environment\n",
        "You also need to install PyTorch and torchvision, which are the libraries used in this tutorial.\n",
        "\n",
        "You can install them using `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets"
      ],
      "metadata": {
        "id": "j3Z_Ao_9uSOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚¨áÔ∏è Download and Prepare the Dataset\n",
        "\n",
        "You'll use the [MiniPlaces dataset](https://github.com/CSAILVision/miniplaces) for this tutorial.\n",
        "\n",
        "In typical image classification datasets, the folder structure is usually organized in a way that each class of images is stored in its own sub-folder.\n",
        "\n",
        "This is commonly referred to as the \"Image Folder\" structure.\n",
        "\n",
        "When you've downloaded and extracted the MiniPlaces dataset, you'll find the images organized into folders according to their respective classes.\n",
        "\n",
        "Image folder format looks like this:\n",
        "\n",
        "```\n",
        "miniplaces/\n",
        "|-- train/\n",
        "|   |-- class1/\n",
        "|   |   |-- image1.jpg\n",
        "|   |   |-- image2.jpg\n",
        "|   |   |-- ...\n",
        "|   |-- class2/\n",
        "|   |   |-- image1.jpg\n",
        "|   |   |-- image2.jpg\n",
        "|   |   |-- ...\n",
        "|   |-- ...\n",
        "|-- val/\n",
        "|   |-- class1/\n",
        "|   |   |-- image1.jpg\n",
        "|   |   |-- image2.jpg\n",
        "|   |   |-- ...\n",
        "|   |-- class2/\n",
        "|   |   |-- image1.jpg\n",
        "|   |   |-- image2.jpg\n",
        "|   |   |-- ...\n",
        "|   |-- ...\n",
        "```\n",
        "\n",
        "### ü™ú The following code performs several steps:\n",
        "\n",
        "1. **Download and Extract Dataset:** Downloads the MiniPlaces dataset and extracts it to the 'miniplaces' directory.\n",
        "\n",
        "2. **Setup Paths:** Sets up paths for the main dataset, the validation set, and creates a new directory for the test set.\n",
        "\n",
        "3. **List Classes:** Lists all the classes in the dataset by iterating over the directories in the validation set.\n",
        "\n",
        "4. **Split Validation Set:** For each class, splits the images in the validation set into a smaller validation set and a new test set using a 60/40 split.\n",
        "\n",
        "5. **Move Images:** Moves the images for the test set from the validation directory to the test directory, maintaining the original directory structure."
      ],
      "metadata": {
        "id": "N1Jyy5v-tcRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://dissect.csail.mit.edu/datasets/miniplaces.zip --no-check-certificate && unzip miniplaces.zip"
      ],
      "metadata": {
        "id": "c5QDDdy3zKzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "# set the dataset path\n",
        "dataset_path = Path('miniplaces')\n",
        "val_path = dataset_path / 'val'\n",
        "\n",
        "# get the list of classes\n",
        "classes = [d.name for d in val_path.iterdir() if d.is_dir()]\n",
        "\n",
        "# create a test directory\n",
        "test_path = dataset_path / 'test'\n",
        "test_path.mkdir(exist_ok=True)\n",
        "\n",
        "# iterate over each class\n",
        "for class_name in classes:\n",
        "    class_dir = val_path / class_name\n",
        "    class_files = [f.name for f in class_dir.iterdir() if f.is_file()]\n",
        "\n",
        "    # split the validation set\n",
        "    val_files, test_files = train_test_split(class_files, test_size=0.4, random_state=42)\n",
        "\n",
        "    # create new test class directory\n",
        "    test_class_dir = test_path / class_name\n",
        "    test_class_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # move test files from validation folder to test folder\n",
        "    for test_file in test_files:\n",
        "        source = class_dir / test_file\n",
        "        destination = test_class_dir / test_file\n",
        "        shutil.move(source, destination)"
      ],
      "metadata": {
        "id": "UFCC_HOYv0xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíæ Dataset and Dataloader\n",
        "In PyTorch, datasets and dataloaders are essential components for efficiently working with large-scale datasets during the training process.\n",
        "\n",
        "1) **Dataset**: A Dataset in PyTorch represents a collection of data. It could be in-memory data like a list of Python objects or out-of-memory data like images on a disk. The Dataset class provides a standardized way to access this data. You typically subclass the Dataset class and implement the `__getitem__` and `__len__` methods to return a single data item and the size of the dataset, respectively. PyTorch also provides several pre-defined Dataset classes like ImageFolder for common use-cases.\n",
        "\n",
        "2) **DataLoader**: While the Dataset class provides a way to access individual data items, the DataLoader class provides a way to iterate over a Dataset in batches. This is important because when training a neural network, you typically don't feed in the entire dataset at once but process it in smaller batches. The DataLoader also makes it easy to shuffle the data and use multiple subprocesses to load the data in parallel.\n",
        "\n",
        "To create datasets and dataloaders for the MiniPlaces dataset, you can use PyTorch's `ImageFolder` and `DataLoader` classes.\n",
        "\n",
        "The `ImageFolder` class allows you to load the images directly from the directory structure, and the `DataLoader` class provides an efficient way to iterate over these images during training.\n"
      ],
      "metadata": {
        "id": "8x60KI9_tvEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# define the transform\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # resize images to 224x224\n",
        "    transforms.ToTensor(),  # convert image to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # normalize images to ImageNet mean and sd\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # resize images to 224x224\n",
        "    transforms.ToTensor(),  # convert image to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # normalize images to ImageNet mean and sd\n",
        "])\n",
        "\n",
        "# define the datasets\n",
        "train_dataset = datasets.ImageFolder(dataset_path / 'train', transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(dataset_path / 'val', transform=test_transform)\n",
        "test_dataset = datasets.ImageFolder(dataset_path / 'test', transform=test_transform)\n",
        "\n",
        "# define the dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "z6sa13Nqt55u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYZw6UvePBv5"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/Deci-AI/super-gradients/master/documentation/assets/SG_img/SG%20-%20Horizontal%20Glow%202.png'>\n",
        "\n",
        "# ü¶∏üèæ‚Äç‚ôÇÔ∏è SuperGradients\n",
        "\n",
        "SuperGradients is a PyTorch based training library.\n",
        "\n",
        "It provides a uniform interface for the most common computer vision use cases:\n",
        "\n",
        "- Classification\n",
        "\n",
        "- Detection\n",
        "\n",
        "- Segmentation\n",
        "\n",
        "- Pose estimation\n",
        "\n",
        "There are nearly 40 pretrained models in the model zoo. You can see the pretrained models available to you by following [this link](https://github.com/Deci-AI/super-gradients/blob/master/documentation/source/model_zoo.md).\n",
        "\n",
        "This notebook will focus on using SuperGradients for image classification."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install super-gradients==3.2.0"
      ],
      "metadata": {
        "id": "e0nvJB5g02CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93O99TjcoANx"
      },
      "source": [
        "# ü™° Fine-tuning on a custom dataset\n",
        "\n",
        "## üèãüèΩ The trainer\n",
        "\n",
        "The first thing you need to define in SuperGradients is the Trainer.\n",
        "\n",
        "The trainer is in charge of training, evaluation, saving checkpoints, etc. If you're interested in seeing the source code for the trainer, you can do so [here](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/sg_trainer/sg_trainer.py).\n",
        "\n",
        "### ‚úåüèº There's two important arguments to the trainer:\n",
        "\n",
        "1) `ckpt_root_dir` - this is the directory where results from all your experiments will be saved\n",
        "\n",
        "2)`experiment_name` - all checkpoints, logs, and tensorboards will be saved in a directory with the name you specify here.\n",
        "\n",
        "SuperGradients supports **Data Parallel** and **Distributed Data Parallel**.\n",
        "\n",
        "That's outside of the scope for this introduction to SuperGradients.\n",
        "\n",
        "But, if you're fortunate enough to have multiple GPUs at your disposal or want learn more you can do so [here](https://github.com/Deci-AI/super-gradients/blob/0fe46cd39572db34eb83d68e343fed97b8886fe9/documentation/source/device.md#3-dp---data-parallel).\n",
        "\n",
        "In the code below, you'll instantiate the trainer with just a single GPU (since that's what Google Colab provides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMtzYc_CoAX3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from super_gradients.training import Trainer\n",
        "CHECKPOINT_DIR = 'checkpoints'\n",
        "trainer = Trainer(experiment_name='hello_world', ckpt_root_dir=CHECKPOINT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW4Qg5YuR0mD"
      },
      "source": [
        "# üë©üèΩ‚Äçü¶≥ Instantiating the model\n",
        "\n",
        "Below is how to instantiate the model for finetuning. Note you need to add the `num_classes` argument here. SG will automatically replace the classification head to match the number of classes you're working with\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iZihVp_dlwr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from super_gradients.training import models\n",
        "model = models.get('resnet18',\n",
        "                   num_classes=len(train_dataset.classes),\n",
        "                   pretrained_weights=\"imagenet\"\n",
        "                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTe5a7-7efZR"
      },
      "source": [
        "# üéõÔ∏è Training parameters\n",
        "\n",
        "You need to define the training parameters for your training run.\n",
        "\n",
        "Full details about the training parameters can be found [here](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/recipes/training_hyperparams/default_train_params.yaml).\n",
        "\n",
        "# üö® There are a few **mandatory** arguments that you must define for training params üö®\n",
        "\n",
        "- `initial_lr` - The initial learning rate for training\n",
        "\n",
        "The learning rate is a hyperparameter that determines the step size at which the model's weights are updated during training.\n",
        "\n",
        "It controls how much the model's weights should be adjusted with respect to the loss gradient.\n",
        "\n",
        "A high learning rate may cause the training to converge quickly, but it can overshoot the optimal solution or cause instability.\n",
        "\n",
        "A low learning rate may lead to more precise convergence to the optimal solution but can be very slow and possibly get stuck in local minima.\n",
        "\n",
        "It balances the speed of convergence and the stability of learning, and it often requires careful tuning to find the optimal value for a given problem.\n",
        "\n",
        "- `max_epochs` - Max number of training epochs\n",
        "\n",
        "An epoch refers to one complete pass through the entire training dataset.\n",
        "\n",
        "During an epoch, the model's weights are updated as it processes each batch of samples in the training dataset.\n",
        "\n",
        "Once every sample has been seen and used to update the weights, one epoch is completed.\n",
        "\n",
        "- `loss` - the loss function you want to use\n",
        "\n",
        "- `optimizer` - Optimizer you will be using\n",
        "\n",
        "- `train_metrics_list` - Metrics to log during training\n",
        "\n",
        "- `valid_metrics_list` - Metrics to log during validation\n",
        "\n",
        "- `metric_to_watch` - metric which the model checkpoint will be saved according to\n",
        "\n",
        "You can choose from a variety of `optimizer`'s such as: Adam, AdamW, SGD, Lion, or RMSProps. If you choose to change the defualt parameters of these optimizrs you pass them into `optimizer_params`.\n",
        "\n",
        "### Train/val metrics definitions:\n",
        "\n",
        "**Accuracy** -  known as Top-1 accuracy, this metric tells you the ratio of correct predictions to the total number of items.  It answers the question, \"How often does the model predict the exact correct class?\"\n",
        "\n",
        "**Top-5** - If one of the model‚Äôs top 5 highest probability predictions match the ground truth. If it does, you count it as a correct prediction. It answers the question, \"How often is the correct class within the top 5 guesses of the model?\"\n",
        "\n",
        "Accuracy is often used for tasks where the exact prediction is crucial. Top-5 Accuracy is more common in scenario where there may be ambiguity, and being close to the correct answer is still valuable.\n",
        "\n",
        "# Optional, but good to know parameters\n",
        "\n",
        "### `optimizer_params` - The parameters of the optimizer you're using, leaving it blank will use defaults\n",
        "\n",
        "[For example, there are the parameters for Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
        "\n",
        "\n",
        "### `criterion_params` - Activates label smoothing cross entropy\n",
        "\n",
        "In the regular cross entropy loss, we're very strict about our labels: the correct class has a value of 1, and all other classes have a value of 0.\n",
        "\n",
        "But this can sometimes lead to overfitting, where the model becomes too confident about its predictions and performs poorly on unseen data.\n",
        "\n",
        "Label smoothing cross-entropy builds upon regular cross-entropy by adding a touch of \"softness\" to the labels, leading to a model that's confident but not excessively so.\n",
        "\n",
        "**Label Smoothing Cross-Entropy**: Instead of using strict 0s and 1s in the true labels, label smoothing uses slightly \"softer\" values (e.g., [0.9, 0.05, 0.05] for class 1).\n",
        "\n",
        "Improvement:\n",
        " - **Avoiding Overconfidence:** It prevents the model from becoming overly confident about its predictions. It acts as a form of regularization that can improve generalization to unseen data.\n",
        "\n",
        " - **Stability in Training:** Makes the training process more stable, especially when dealing with noisy or incorrect labels.\n",
        "\n",
        "Label smoothing is a simple yet effective technique to make the training process more robust and prevent overfitting.\n",
        "\n",
        "By softening the labels, we encourage the model to be less confident in its predictions, which can lead to better performance on unseen data. It's like telling the model, \"Be confident, but not too confident!\"\n",
        "\n",
        "Smoothing Factor (epsilson): This is a value between 0 and 1 that determines the degree of smoothing applied to the labels.\n",
        "\n",
        "A smoothing factor of 0 means no smoothing (i.e., traditional cross-entropy), while a value closer to 1 means more smoothing.\n",
        "\n",
        "The smoothing factor is a hyperparameter that you would typically choose based on validation performance or through hyperparameter tuning. It allows you to control how \"soft\" the labels are, providing a way to regulate the model's confidence in its predictions.\n",
        "\n",
        "\n",
        "### Learning rate scheduling parameters: `warmup_mode`, `lr_warmup_epochs`, `lr_warmup_steps`, `lr_cooldown_epochs` `warmup_initial_lr`\n",
        "\n",
        "Learning rate scheduling involves adjusting the learning rate during training. Instead of using a constant learning rate, the learning rate is changed according to a predefined schedule or rule.\n",
        "\n",
        "It helps in finding a balance between fast convergence (using a high learning rate) and stability (using a low learning rate). Common methods include step decay, exponential decay, and cosine annealing.\n",
        "\n",
        "- Warm-Up: Starts with a small learning rate and gradually increases it at the beginning of training to avoid large, unstable updates.\n",
        "\n",
        "- Cool-Down: Gradually reduces the learning rate towards the end of training to help the model settle into a minimum.\n",
        "\n",
        "Learning rate scheduling, including warm-up and cool-down, is generally beneficial in the following scenarios:\n",
        "\n",
        "1. Large Datasets: These techniques help the model converge more efficiently.\n",
        "\n",
        "2. Deep Networks: For deep networks with many layers, controlling the learning rate prevents overshooting and helps the model reach a better minimum.\n",
        "\n",
        "3. Avoiding Overfitting: Gradually reducing the learning rate (cool-down) helps in fine-tuning the model and reduces the risk of overfitting.\n",
        "\n",
        "4. Training Stability: Warm-up can prevent large updates at the beginning of training, leading to a more stable training process.\n",
        "\n",
        "5. Exploration vs Exploitation: By controlling the learning rate, you can balance the exploration of the loss landscape (with higher learning rates) and exploitation of promising areas (with lower learning rates).\n",
        "\n",
        "Together, these techniques guide the learning process, aiding in efficient and robust convergence of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TnXgisyefds"
      },
      "outputs": [],
      "source": [
        "train_params = {\n",
        "    \"initial_lr\": 3e-1,\n",
        "    \"max_epochs\":10,\n",
        "    \"loss\":\"cross_entropy\",\n",
        "    \"optimizer\":\"Adam\", # you can choose from Adam, AdamW, SGD, Lion, or RMSProps\n",
        "    \"train_metrics_list\":[\"Accuracy\", \"Top5\"],\n",
        "    \"valid_metrics_list\":[\"Accuracy\", \"Top5\"],\n",
        "    \"metric_to_watch\":\"Accuracy\",  # will be the metric which the model checkpoint will be saved according to\n",
        "\n",
        "    # optional parameters\n",
        "    \"optimizer_params\": {}, # when `optimizer` is one of ['Adam','SGD','RMSProp'], it will be initialized with optimizer_params.\n",
        "    \"criterion_params\": {'smooth_eps': 0.1}, # Enable label-smoothing cross-entropy\n",
        "    \"warmup_initial_lr\": 3e-4, # Initial lr for linear_epoch_step/linear_batch_step. When none is given, initial_lr/(warmup_epochs+1) will be used.\n",
        "    \"warmup_mode\": \"linear_epoch_step\", # learning rate warmup scheme, currently 'linear_epoch_step' and 'linear_batch_step' are supported\n",
        "    \"lr_warmup_epochs\": 3, # number of epochs for learning rate warm up\n",
        "    \"lr_warmup_steps\": 2,  # number of warmup steps\n",
        "    \"lr_cooldown_epochs\": 1 # epochs to cooldown LR (i.e the last epoch from scheduling view point=max_epochs-cooldown)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_params"
      ],
      "metadata": {
        "id": "0B_wpTe3yd0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ü™Ñ SuperGradients offers a number of training tricks right out of the box, such as:\n",
        "\n",
        "- Exponential moving average\n",
        "- Zero weight decay on bias and batch normalizatiom\n",
        "- Weight averaging\n",
        "- Batch accumulation\n",
        "- Precise BatchNorm\n",
        "\n",
        "You can read more details about these training tricks [here](https://heartbeat.comet.ml/a-better-way-to-train-your-neural-networks-813b60a5bd6a).\n",
        "\n",
        "If you're interested in building a using a custom metric with SuperGradients you can learn how [here](https://github.com/Deci-AI/super-gradients/blob/master/documentation/source/Metrics.md).\n"
      ],
      "metadata": {
        "id": "IVstwvNkgyEx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8c38zUUefjo"
      },
      "source": [
        "# ü¶æ Training the model\n",
        "\n",
        "You've covered a lot of ground so far:\n",
        "\n",
        "‚úÖ Instantiated the trainer\n",
        "\n",
        "‚úÖ Defined your dataset parameters and dataloaders\n",
        "\n",
        "‚úÖ Instantiated a model\n",
        "\n",
        "‚úÖ Set up your training parameters\n",
        "\n",
        "### ‚è≥ Now, its time to train a model\n",
        "\n",
        "Training a model using a SuperGradients is done using the `trainer`.\n",
        "\n",
        "It's as easy as..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5sKiaH9efp-"
      },
      "outputs": [],
      "source": [
        "trainer.train(model=model,\n",
        "              training_params=train_params,\n",
        "              train_loader=train_loader,\n",
        "              valid_loader=val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sP59sDjefzK"
      },
      "source": [
        "# üèÜ Get the best trained model\n",
        "\n",
        "Now that training is complete, you need to get the best trained model.\n",
        "\n",
        "You used checkpoint averaging so the following code will use weights averaged across training runs.\n",
        "\n",
        "If you want to use the best weights, or weights from the last epoch you'd use one of the following in the code below:\n",
        "\n",
        "- best weights: `checkpoint_path = content/checkpoints/hello_world/ckpt_best.pth`\n",
        "\n",
        "- last weights: `checkpoint_path = content/checkpoints/hello_world/ckpt_latest.pth`\n",
        "\n",
        "- average weights: `checkpoint_path = content/checkpoints/hello_world/average_model.pth`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cXthCX2vgAC"
      },
      "outputs": [],
      "source": [
        "best_model = models.get('resnet18',\n",
        "                        num_classes=len(train_dataset.classes),\n",
        "                        checkpoint_path=\"/content/checkpoints/hello_world/best_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gRaxH4dxJAx"
      },
      "source": [
        "# üßê Evaluating the best trained model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(model=best_model,test_loader=test_loader)"
      ],
      "metadata": {
        "id": "d112RM240zqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPdjT8Crw9UR"
      },
      "source": [
        "# üîÆ Predicting with the best model\n",
        "\n",
        "The next line will perform detection on the following image. Note, we didn't have a class for the half dollar coin. So it will likely get classified as something else.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from super_gradients.training.processing.processing import default_imagenet_processing_params\n",
        "processing_params = default_imagenet_processing_params()\n",
        "processing_params['class_names'] = train_dataset.classes\n",
        "best_model.set_dataset_processing_params(**processing_params)"
      ],
      "metadata": {
        "id": "2EUq50prW1FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG3XCOYfw9d5"
      },
      "outputs": [],
      "source": [
        "img_url = '/content/miniplaces/test/aquarium/00000248.jpg'\n",
        "best_model.predict(img_url).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí´ I think you're ready to venture out on your own now!\n",
        "\n",
        "I've created a templated notebook for you [here](https://colab.research.google.com/drive/1HWU6oOEVfh3sS59KHH3q6StG1ZtO1D2Q).\n",
        "\n",
        "If you run into any issues, you know how to get a hold of me (contact info is at the top of the notebook).\n",
        "\n",
        "Cheers and I can't wait to see what you come up with!"
      ],
      "metadata": {
        "id": "p4kz-ucAPB3r"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}