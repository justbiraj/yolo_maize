{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Training a model to perform at its best can be a challenging task.\n",
        "\n",
        "Having explored the intricacies of various training techniques, such as Precise BatchNorm, Weight Averaging, and Batch Accumulation, you've learned how these can significantly improve a model's performance and stability.\n",
        "\n",
        "Now, it's time to take the next step and see these concepts in action!\n",
        "\n",
        "In the SuperGradients framework, implementing these training tricks is straightforward and efficient.\n",
        "\n",
        "Let's dive into the code and demonstrate just how easy it is to take advantage of these powerful techniques within your own models.\n"
      ],
      "metadata": {
        "id": "mfgDTx1b0wlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install super-gradients==3.2.0"
      ],
      "metadata": {
        "id": "FewPYAYPnUmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from super_gradients.training import training_hyperparams"
      ],
      "metadata": {
        "id": "BfGZs--zpIDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = training_hyperparams.get(\"training_hyperparams/default_train_params\")"
      ],
      "metadata": {
        "id": "4MOJZUlMpOgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exponential Moving Average (EMA)\n",
        "\n",
        "Getting trapped in a false local minima sucks.\n",
        "\n",
        "When you’re training a neural network, chances are you’re using mini-batches.\n",
        "\n",
        "There's nothing wrong with that, it just happens to introduce noise and less accurate gradients when gradient descent updates model parameters between batches.\n",
        "\n",
        "On one hand, thats nice because noisy gradients can sometimes help optimization and lead to a better local optimum than if you trained on the entire data set.\n",
        "\n",
        "On the other hand, the noisiness might lead to converging to a false local minima.\n",
        "\n",
        "Luckily, you have the EMA method at your disposal.\n",
        "\n",
        "EMA is a method that increases the stability of a model’s convergence and helps it reach a better overall solution by preventing convergence to a local minima.\n",
        "\n",
        "EMA makes your models more stable, improves convergence, and helps your network find a better solution."
      ],
      "metadata": {
        "id": "WJ_Zxv3NmgLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['ema']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azzR5Yi2mgQY",
        "outputId": "9f6358bc-d8b9-4027-dfd3-8927fc0e2a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['ema'] = 'True'"
      ],
      "metadata": {
        "id": "BCO_Pj8GmgWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['ema_params']"
      ],
      "metadata": {
        "id": "-nxGHSzhrYAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight Averaging\n",
        "\n",
        "Everyone likes a free boost in model accuracy.\n",
        "\n",
        "And that's what weight averaging gives you.\n",
        "\n",
        "It’s a post-training method that takes the best model weights across training epochs and averages them into a single model.\n",
        "\n",
        "By averaging weights for the N best checkpoints, we’re effectively making an ensemble of N models.\n",
        "\n",
        "It is not exactly the same as having N models and averaging their predictions, which comes at the price of running inference on N models, but it could help with squeezing out some extra accuracy.\n",
        "\n",
        "It does this by overcoming the optimization tendency to alternate between adjacent local minima in the later stages of the training.\n",
        "\n",
        "It also has the added benefit of reducing bias.\n",
        "\n",
        "This trick doesn’t affect the training at all, it just keeps a few additional weights on the disk, and can give you a boost in performance and stability."
      ],
      "metadata": {
        "id": "GG4QVy1umgbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = training_hyperparams.get(\"training_hyperparams/default_train_params\")"
      ],
      "metadata": {
        "id": "kv1APl7ymggP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['average_best_models']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXe-FhLXmglh",
        "outputId": "5145fd52-eb93-4a35-c130-0f922dfdf851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['average_best_models'] = 'False'"
      ],
      "metadata": {
        "id": "9NzjazlWrzV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Accumulation\n",
        "\n",
        "Most \"off-the-shelf’ models come with a suggested training recipe.\n",
        "\n",
        "Which usually suggests a powerful GPU for training.\n",
        "\n",
        "If you just try to reduce your batch size so it works with your hardware, you’ll have to tune other parameters as well.\n",
        "\n",
        "Which means you won’t always get the same training results.\n",
        "\n",
        "There’s got to be a way to train a model thats appropriate for your target hardware.\n",
        "\n",
        "That’s where batch accumulation comes it.\n",
        "\n",
        "Here’s how it works…\n",
        "\n",
        "1) Perform several consecutive forward steps over the model\n",
        "\n",
        "2) Accumulate the gradients\n",
        "\n",
        "3) Back propagate them once every few batches.\n",
        "\n",
        "Be sure that you are training with a small batch size to begin with, typically 4 or 8 should be good for most smaller GPUs.\n",
        "\n",
        "Next, determine the virtual batch size you want to simulate.\n",
        "\n",
        "If you're working with a batch size of 4 but want to simulate a batch size of 64 then you'd accumulate for 64/4 = 16 batches."
      ],
      "metadata": {
        "id": "OJqRc71Wmgqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = training_hyperparams.get(\"training_hyperparams/default_train_params\")"
      ],
      "metadata": {
        "id": "bPNnWypnmgvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['batch_accumulate'] = 16"
      ],
      "metadata": {
        "id": "9j67_Dd9mwS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precise BatchNorm\n",
        "\n",
        "BatchNorm is a wonderful invention.\n",
        "\n",
        "Ever since it hit the scene in 2015 its been making models less sensitive to learning rates and choice of initialization.\n",
        "\n",
        "It’s also helped speed up model convergence.\n",
        "\n",
        "Hell, it’s even helped wage war against overfitting.\n",
        "\n",
        "\n",
        "BatchNorm does, however, have it’s problems…\n",
        "\n",
        "\n",
        "> Batch normalization in the mind of many people, including me, is a necessary evil. In the sense that nobody likes it, but it kind of works, so everybody uses it, but everybody is trying to replace it with something else because everybody hates it.\n",
        "\n",
        "> — Yann LeCun\n",
        "\n",
        "Why does BatchNorm catch such flack?\n",
        "\n",
        "Well, BatchNorm layers are meant to normalize the data based on the dataset’s distribution.\n",
        "\n",
        "Ideally, you want to estimate the distribution according to the entire dataset.\n",
        "\n",
        "But this isn’t possible.\n",
        "\n",
        "So, BatchNorm layers are used to evaluate the statistics of a given mini-batch throughout the training.\n",
        "\n",
        "But a 2021 paper by Facebook AI Research titled \"Rethinking “Batch” in BatchNorm” showed that these mini-batch based statistics are sub-optimal.\n",
        "\n",
        "The researchers propose estimating the data statistics parameters (the mean and standard deviation variables) across several mini-batches, while keeping the trainable parameters fixed.\n",
        "\n",
        "This method, titled Precise BatchNorm, helps improve both the stability and performance of a model.\n",
        "\n",
        "If you want to use PreciseBN it's preferabe set batch size to something small, mimicking a scenario where we can't fit large batch in your GPU.\n",
        "\n",
        "Then set `precie_bn_batch_size` to be large enough until you see some good results"
      ],
      "metadata": {
        "id": "Or7FpLwkmwnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = training_hyperparams.get(\"training_hyperparams/default_train_params\")"
      ],
      "metadata": {
        "id": "ns2P5xT1mwwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['precise_bn'] = 'True'\n",
        "# the effective batch size we want to calculate the batchnorm on.\n",
        "training_params['precise_bn_batch_size'] = 256"
      ],
      "metadata": {
        "id": "Bdftm5oEmg01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-weight decay on BatchNorm and Bias\n",
        "\n",
        "Most computer vision tasks have BatchNorm layers and biases along with linear or convolutional layers.\n",
        "\n",
        "This tends to work well because you’ll have more parameters in your model.\n",
        "\n",
        "More parameters mean more ways to capture interactions between parts of your network.\n",
        "\n",
        "More parameters, however, also mean more opportunities to overfit your model."
      ],
      "metadata": {
        "id": "8yR_QVt2mz4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = training_hyperparams.get(\"training_hyperparams/default_train_params\")"
      ],
      "metadata": {
        "id": "djnzWG2_m3py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params['zero_weight_decay_on_bias_and_bn'] = 'True'"
      ],
      "metadata": {
        "id": "cLj4o3FWxVJ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}